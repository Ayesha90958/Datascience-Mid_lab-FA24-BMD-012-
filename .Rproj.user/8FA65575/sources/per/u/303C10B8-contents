# ------------------------------
# Task 1: Data Import
# ------------------------------

# Load required libraries
library(tidyverse)
library(readr)

# Import the dataset (update file path if needed)
air_data <- read_csv("openaq_location_1214597_measurments.csv")

# View structure and summary
str(air_data)
summary(air_data)

# Number of rows and columns
cat("Rows:", nrow(air_data), "\n")
cat("Columns:", ncol(air_data), "\n")

# Number of missing values per column
colSums(is.na(air_data))
# ------------------------------
# Task 2: Data Cleaning
# ------------------------------

library(lubridate)

# 2a. Remove duplicate records
air_data <- air_data %>% distinct()
cat("Duplicates removed. Remaining rows:", nrow(air_data), "\n")

# 2b. Rename columns for clarity
colnames(air_data)
# (Based on what columns your dataset has â€” let's rename them clearly)
colnames(air_data) <- c("Location", "City", "Country", "Parameter", "Value", "Unit", "Date_UTC", "Latitude", "Longitude")

# 2c. Convert data types
air_data$City <- as.factor(air_data$City)
air_data$Country <- as.factor(air_data$Country)
air_data$Parameter <- as.factor(air_data$Parameter)
air_data$Date_UTC <- ymd_hms(air_data$Date_UTC)
air_data$Value <- as.numeric(air_data$Value)

# 2d. Handle missing values
# Remove rows with missing important columns
air_data <- air_data %>% drop_na(Location, City, Country, Parameter, Value)

# 2e. Detect and handle outliers using IQR
Q1 <- quantile(air_data$Value, 0.25, na.rm = TRUE)
Q3 <- quantile(air_data$Value, 0.75, na.rm = TRUE)
IQR_val <- Q3 - Q1

# Remove outliers
air_data <- air_data %>%
  filter(Value >= (Q1 - 1.5*IQR_val) & Value <= (Q3 + 1.5*IQR_val))

cat("Outliers handled. Remaining rows:", nrow(air_data), "\n")

# View cleaned data
glimpse(air_data)
install.packages("tidyverse")
names(air_data)
colnames(air_data) <- c("Location", "City", "Country", "Parameter", "Value", "Unit", "Date_UTC")
air_data <- air_data %>% drop_na(Location, City, Country, Parameter, Value)
names(air_data)[names(air_data) == "" | is.na(names(air_data))] <- paste0("Unknown_", seq_len(sum(names(air_data) == "" | is.na(names(air_data)))))
# ------------------------------
# Descriptive Statistics
# ------------------------------

# Select major pollutants if available
pollutants <- c("pm25", "pm10", "no2", "co", "so2")

descriptive_stats <- air_data %>%
  filter(Parameter %in% pollutants) %>%
  group_by(Parameter) %>%
  summarise(
    Mean = mean(Value, na.rm = TRUE),
    Median = median(Value, na.rm = TRUE),
    Variance = var(Value, na.rm = TRUE),
    SD = sd(Value, na.rm = TRUE)
  )

print(descriptive_stats)
# ------------------------------
# Correlation Matrix
# ------------------------------

library(ggcorrplot)

# Convert long to wide format
air_wide <- air_data %>%
  filter(Parameter %in% pollutants) %>%
  group_by(Date_UTC) %>%
  pivot_wider(names_from = Parameter, values_from = Value, values_fn = mean)

# Compute correlation
cor_matrix <- cor(air_wide %>% select(-Date_UTC), use = "complete.obs")
ggcorrplot(cor_matrix, lab = TRUE, title = "Correlation Heatmap of Pollutants")


# ------------------------------
# Visual Analysis
# ------------------------------

library(ggplot2)

# Histogram for PM2.5
ggplot(air_data %>% filter(Parameter == "pm25"), aes(x = Value)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
  labs(title = "Distribution of PM2.5 Levels", x = "PM2.5", y = "Frequency")

# Boxplot for pollutants by City
ggplot(air_data %>% filter(Parameter %in% pollutants), 
       aes(x = City, y = Value, fill = Parameter)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Pollutant Variation Across Cities", y = "Concentration")

# Time-series for PM2.5 trend
air_pm25 <- air_data %>%
  filter(Parameter == "pm25") %>%
  group_by(Date_UTC) %>%
  summarise(Avg_PM25 = mean(Value, na.rm = TRUE))

ggplot(air_pm25, aes(x = Date_UTC, y = Avg_PM25)) +
  geom_line(color = "red") +
  labs(title = "PM2.5 Concentration Trend Over Time", x = "Date", y = "Average PM2.5")

